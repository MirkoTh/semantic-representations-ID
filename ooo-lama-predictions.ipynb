{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a530060b-d694-412f-97b6-bc44b3dc358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f324cb-76a3-4f9d-8188-3713186533f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import os\n",
    "from functools import partial\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3357d-15a6-435f-9636-16766b9e9754",
   "metadata": {},
   "source": [
    "class Unsloth:\n",
    "    def __init__(self, llm_info):\n",
    "        engine, max_tokens, temperature = llm_info\n",
    "        engine = engine.replace('unsloth_', 'unsloth/')\n",
    "\n",
    "        print(engine)\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = engine,\n",
    "            max_seq_length = max_tokens,\n",
    "            dtype = None,\n",
    "            load_in_4bit = True,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model) # just add this line and solve the error `Cache only has 0 layers...`\n",
    "        self.pipe = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            temperature = temperature,# +1e-6, # 1 \n",
    "            max_new_tokens = 20 #2 #TODO: HARCODED FOR NOW (1 token at a time)-maybe have to adapt for probabilistic reasoning\n",
    "        )\n",
    "\n",
    "    def _generate(self, text, temp, max_tokens):\n",
    "        r = self.pipe(text)[0]\n",
    "        print(r)\n",
    "        response = r['generated_text'][len(text):]\n",
    "        # response = response.replace(' ', '')            \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e62b29-5169-45d6-b25f-d0c461c92bd4",
   "metadata": {},
   "source": [
    "model_id = (\"meta-llama/Llama-3.1-8B-Instruct\", 32768, +1e-6) #+1e-6\n",
    "model = Unsloth(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc979c13-2904-4b45-82c9-8f2fa7c022da",
   "metadata": {},
   "source": [
    "# take out from_pretrained etcl... code milena\n",
    "- for consistent responses, use 70B\n",
    "- how to request 2 gpus use can's wiki\n",
    "- between these two gpus, the 70B model can be run (required for memory storage)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b2bfc0-0cc2-40de-a0e5-cef98ab2598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: Tesla V100-SXM3-32GB. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 7.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device does not support bfloat16. Will change to float16.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e68f96c65ba48eaaa326094109ababe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.1-8B-Instruct does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
     ]
    }
   ],
   "source": [
    "nbilly = 8\n",
    "engine = f\"\"\"meta-llama/Llama-3.1-{nbilly}B-Instruct\"\"\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = engine,\n",
    "            dtype = torch.bfloat16,\n",
    "            load_in_4bit = True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "7ce9e2b1-0196-4c3b-8940-83cd427655d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "305ed4c4-711c-496b-86e9-540856ed0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_input = (\n",
    "    \"You are a cognitive scientist and you are given a by-trial data set from a participant. \" +\n",
    "    \"in the odd-one-out task. For each trial, you are presented with the three words and \" +\n",
    "    \"the participant's response, i.e., the word the participant thinks is the odd one out of the three. \" +\n",
    "    \"Note that in the very first trial, you are only presented with the three words, but no previous responses.\"\n",
    "    \n",
    "    \"\\nYour goal is to predict, for a new triplet of words, \" +\n",
    "    \"which word the person is most likely to identify as the odd one out.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc565b-70a6-40ad-81e1-74daf5cb51a5",
   "metadata": {},
   "source": [
    "## Load Triplet Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a78b0af-4892-45f6-8708-8c6e232b97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1253f818-21ca-4ea8-9e9c-e86caf62f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = torch.from_numpy(np.loadtxt(\n",
    "    pjoin(\"./semantic-representations-ID/data/\", 'train_90_ID.txt'))).to(device).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba1dd88-db4c-4789-80f8-6e5214345820",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_ids = train_triplets.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c79226c-d547-4462-8a1c-83c68b17cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_labels = pd.read_csv(\"./semantic-representations-ID/data/unique_id.txt\", delimiter=\"\\\\\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294871d6-e30f-4a31-93c3-8323bf7caf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_labels[0] = tbl_labels[0].str.replace(r'\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "b978f570-ba45-45c2-8808-405abf3753b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_triplets = triplet_ids[triplet_ids[:, 3] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "304364ed-5152-47e2-bf42-feeb9ff59679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_and_extract_trial(tbl_labels, l_ids):\n",
    "    tbl_select = tbl_labels.iloc[l_ids[0:3]]\n",
    "    tbl_select.reset_index(drop=True, inplace=True)\n",
    "    rand_order = np.random.choice([0, 1, 2], 3, replace=False)\n",
    "    tbl_reordered = tbl_select.iloc[rand_order].reset_index(drop=True)\n",
    "    tbl_reordered[\"rnd\"] = rand_order\n",
    "    tbl_return = pd.DataFrame({\n",
    "        \"1\": [tbl_reordered.iloc[0, 0]],\n",
    "        \"2\": [tbl_reordered.iloc[1, 0]],\n",
    "        \"3\": [tbl_reordered.iloc[2, 0]],\n",
    "        \"selected\": np.where(tbl_reordered[\"rnd\"] == 2)[0] + 1\n",
    "    })\n",
    "    return tbl_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "820d1b9b-bf68-402f-92d5-d2401885ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomize_and_extract_partial = partial(randomize_and_extract_trial, tbl_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "9df1bb9d-506e-49d9-b4d8-66cec3318054",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_trials = list(map(randomize_and_extract_partial, triplet_ids[triplet_ids[:,3]==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "93444cb5-e5ea-4e06-b7ee-ddcaff9a0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.concat(l_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "86363a68-0a76-4fe6-bf77-404f81dac918",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_all_current_tasks = []\n",
    "l_all_previous_trials = []\n",
    "n_trials = 100\n",
    "for current_trial_id in range(0, n_trials):\n",
    "    l_previous_trials = []\n",
    "    current_trial = df_trials.iloc[current_trial_id, :]\n",
    "    current_task = \", \".join([f\"{index+1}. {item}\" for index, item in enumerate(current_trial[0:3])])\n",
    "    if current_trial_id > 0:\n",
    "        for previous_trial_id in range(0, (current_trial_id)):\n",
    "            previous_trial = df_trials.iloc[previous_trial_id, :]\n",
    "            response_prev = f\"\"\"{previous_trial[\"selected\"]}. {previous_trial.iloc[previous_trial[\"selected\"]-1]} in: \"\"\" + \", \".join([f\"{index+1}. {item}\" for index, item in enumerate(previous_trial[0:3])])\n",
    "            l_previous_trials.append(response_prev)\n",
    "    l_all_current_tasks.append(current_task)\n",
    "    l_all_previous_trials.append(l_previous_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "6a1ad6ad-00a0-4f6c-85fa-ccfafe2b802c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba24767243943e09f739a6050d58f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_responses_int = []\n",
    "for trial_id in tqdm(range(0, n_trials)):\n",
    "    l_prev_concat = l_all_previous_trials[trial_id]\n",
    "    current = l_all_current_tasks[trial_id]\n",
    "    input_text = (\n",
    "        \"In an odd-one-out task, a participant is presented with three objects in each trial.\\n\" +\n",
    "        \"The participant is instructed to select the object they think is most dissimilar from the other presented objects.\\n\" +\n",
    "        \"A participant's response is neither correct nor incorrect. The response should simply reflect the participant's \" +\n",
    "        \"perceived semantic similarity between the three objects.\"\n",
    "    )\n",
    "    \n",
    "    if trial_id == 0:\n",
    "        question_text = (\n",
    "            f\"\"\"\\nThe current trial is trial nr. {trial_id + 1}\"\"\"\n",
    "        \"\\nWhich of the following three words is the person to denote as the odd-one-out?\\n\"\n",
    "        f\"\"\"{current}?\"\"\"\n",
    "        )\n",
    "    elif trial_id > 0:\n",
    "        question_text = (\n",
    "            f\"\"\"\\nThe current trial is trial nr. {trial_id + 1}\"\"\"\n",
    "            \"\\nHere are the previous responses from the participant:\\n\" +\n",
    "            \"\\n\".join(l_prev_concat) +\n",
    "            \"\\nWhich of the following three words is the person to denote as the odd-one-out?\\n\"\n",
    "            f\"\"\"{current}?\"\"\"\n",
    "            )\n",
    "    \n",
    "    user_input = input_text + question_text\n",
    "    \n",
    "    l_prev_concat = l_all_previous_trials[trial_id]\n",
    "    current = l_all_current_tasks[trial_id]\n",
    "    input_text = (\n",
    "        \"In an odd-one-out task, a participant is presented with three objects in each trial. \" +\n",
    "        \"The participant is instructed to select the object they think is most dissimilar from the other presented objects. \" +\n",
    "        \"A participant's response is neither correct nor incorrect. The response should simply reflect the participant's \" +\n",
    "        \"perceived semantic similarity between the three objects.\"\n",
    "    )\n",
    "    \n",
    "    question_text = (\n",
    "        \" Here are the previous responses from one participant:\\n\" +\n",
    "        \"\\n\".join(l_prev_concat) +\n",
    "        \"\\nWhich of the following three words is the person to denote as the odd-one-out?\\n\"\n",
    "        f\"\"\"{current}?\"\"\"\n",
    "        )\n",
    "    \n",
    "    user_input = input_text + question_text\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_input}, \n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        {\"role\": \"system\", \"content\": \"For the newest triplet, the participant responds with Nr. \"}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        #add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        continue_final_message=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    \n",
    "    outputs = model.generate(\n",
    "     input_ids,\n",
    "     max_new_tokens=2,\n",
    "     eos_token_id=terminators,\n",
    "     do_sample=False,\n",
    "     #temperature=0.000001\n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    decoded_response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    resp_extract = re.search(\"[1-3]\", decoded_response)\n",
    "    \n",
    "    resp_try = resp_extract.group()\n",
    "    \n",
    "    try:\n",
    "        resp_int = int(resp_try)\n",
    "    except:\n",
    "        resp_int = np.nan\n",
    "\n",
    "    l_responses_int.append(resp_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "e65e989b-d5b7-47f3-99a2-46b07b57b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials_pred = df_trials.head(n_trials).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "8601fd5e-b44b-407b-9cca-9ab7ab6e8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials_pred[\"predicted\"] = l_responses_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "d5f28aa0-422a-418a-8f0c-7e5d836fb6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trials_pred.query(\"selected == predicted\").shape[0] / df_trials_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490914c8-fcb0-4457-b34e-6c12f32be1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb424475-2bf3-44d8-b4ff-67af72a1614c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e85c6-3257-47c2-b2b7-863e29453d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52030557-29fc-42aa-87ef-8c3e3891a5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97c7f843-fca8-4db1-a64a-222de233790f",
   "metadata": {},
   "source": [
    "open ai embedding submodule for embeddings?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
